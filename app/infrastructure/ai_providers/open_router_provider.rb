# frozen_string_literal: true

module Infrastructure
  module AiProviders
    class OpenRouterProvider < BaseProvider
      API_BASE_URL = 'https://openrouter.ai/api/v1'
      
      # Model configurations for 3-tier system
      TIER_MODELS = {
        'tier1' => {
          model: 'mistralai/mistral-7b-instruct',
          name: 'Mistral Small 3.1',
          input_price: 0.00015,  # $0.15 per 1M tokens
          output_price: 0.00015,
          max_tokens: 4000,
          quality_threshold: 0.75
        },
        'tier2' => {
          model: 'meta-llama/llama-3.1-70b-instruct',
          name: 'Llama 4 Maverick',
          input_price: 0.00039,  # $0.39 per 1M tokens
          output_price: 0.00039,
          max_tokens: 8000,
          quality_threshold: 0.85
        },
        'tier3' => {
          model: 'openai/gpt-4o-mini',
          name: 'GPT-4.1 Mini',
          input_price: 0.00040,  # $0.40 per 1M tokens
          output_price: 0.00160,  # $1.60 per 1M tokens
          max_tokens: 16000,
          quality_threshold: 0.95
        }
      }.freeze

      def initialize
        super
        @http_client = HTTParty
      end

      def generate_response(prompt:, max_tokens: 1000, temperature: 0.7, model: nil, tier: 'tier1', images: nil)
        tier = tier.to_s
        model_config = TIER_MODELS[tier]
        
        unless model_config
          return Common::Result.failure(
            Common::Errors::AIProviderError.new(
              provider: 'openrouter',
              message: "Invalid tier: #{tier}. Valid tiers: #{TIER_MODELS.keys.join(', ')}"
            )
          )
        end

        # Use tier-specific model if no model specified
        selected_model = model || model_config[:model]
        
        # Estimate tokens for rate limiting
        estimated_tokens = estimate_tokens(prompt)
        check_rate_limits(estimated_tokens)

        Rails.logger.info("OpenRouter API request - Model: #{selected_model}, Tier: #{tier}, Estimated tokens: #{estimated_tokens}")

        with_retries do
          make_api_request(
            prompt: prompt,
            model: selected_model,
            max_tokens: [max_tokens, model_config[:max_tokens]].min,
            temperature: temperature,
            images: images,
            tier: tier
          )
        end
      end

      def get_model_for_tier(tier)
        TIER_MODELS[tier.to_s]
      end

      def get_tier_models
        TIER_MODELS
      end

      def calculate_cost(input_tokens, output_tokens, tier)
        model_config = TIER_MODELS[tier.to_s]
        return 0.0 unless model_config

        input_cost = (input_tokens / 1_000_000.0) * model_config[:input_price]
        output_cost = (output_tokens / 1_000_000.0) * model_config[:output_price]
        
        input_cost + output_cost
      end

      private

      def make_api_request(prompt:, model:, max_tokens:, temperature:, images: nil, tier: 'tier1')
        request_body = build_request_body(prompt, model, max_tokens, temperature, images)
        
        response = @http_client.post(
          "#{API_BASE_URL}/chat/completions",
          headers: build_headers,
          body: request_body.to_json,
          timeout: 60
        )

        if response.success?
          result = parse_successful_response(response.parsed_response, tier)
          record_api_usage(result.value[:usage][:total_tokens])
          result
        else
          handle_api_error_response(response)
        end
      rescue StandardError => e
        handle_api_error(e, 'openrouter')
      end

      def build_request_body(prompt, model, max_tokens, temperature, images = nil)
        messages = []
        
        if images&.any?
          # Handle multimodal request
          content = [{ type: 'text', text: prompt }]
          
          images.each do |image|
            if image.start_with?('data:image/')
              content << {
                type: 'image_url',
                image_url: { url: image }
              }
            elsif image.start_with?('http')
              content << {
                type: 'image_url',
                image_url: { url: image }
              }
            else
              # File path - convert to base64
              content << {
                type: 'image_url',
                image_url: { url: encode_image_file(image) }
              }
            end
          end
          
          messages << {
            role: 'user',
            content: content
          }
        else
          # Text-only request
          messages << {
            role: 'user',
            content: prompt
          }
        end

        {
          model: model,
          messages: messages,
          max_tokens: max_tokens,
          temperature: temperature,
          top_p: 1.0,
          frequency_penalty: 0.0,
          presence_penalty: 0.0,
          stream: false
        }
      end

      def encode_image_file(file_path)
        return nil unless File.exist?(file_path)
        
        file_extension = File.extname(file_path).downcase
        mime_type = case file_extension
                    when '.png' then 'image/png'
                    when '.jpg', '.jpeg' then 'image/jpeg'
                    when '.webp' then 'image/webp'
                    when '.gif' then 'image/gif'
                    else 'image/jpeg'
                    end
        
        encoded_image = Base64.strict_encode64(File.read(file_path))
        "data:#{mime_type};base64,#{encoded_image}"
      end

      def build_headers
        {
          'Authorization' => "Bearer #{@config[:api_key]}",
          'Content-Type' => 'application/json',
          'HTTP-Referer' => 'https://excelapp.com',
          'X-Title' => 'ExcelApp SaaS'
        }
      end

      def parse_successful_response(response, tier)
        choice = response.dig('choices', 0)
        usage = response['usage']
        
        unless choice && usage
          return Common::Result.failure(
            Common::Errors::AIProviderError.new(
              provider: 'openrouter',
              message: 'Invalid response format from OpenRouter API'
            )
          )
        end

        # Calculate cost
        input_tokens = usage['prompt_tokens'] || 0
        output_tokens = usage['completion_tokens'] || 0
        cost = calculate_cost(input_tokens, output_tokens, tier)

        result = {
          content: choice.dig('message', 'content'),
          usage: {
            prompt_tokens: input_tokens,
            completion_tokens: output_tokens,
            total_tokens: usage['total_tokens'] || (input_tokens + output_tokens)
          },
          model: response['model'],
          tier: tier,
          cost: cost,
          provider: 'openrouter',
          model_config: TIER_MODELS[tier],
          raw_response: response
        }

        unless validate_response(result)
          return Common::Result.failure(
            Common::Errors::AIProviderError.new(
              provider: 'openrouter',
              message: 'Invalid response content'
            )
          )
        end

        Common::Result.success(result)
      end

      def handle_api_error_response(response)
        error_message = response.parsed_response&.dig('error', 'message') || 'Unknown API error'
        
        Rails.logger.error("OpenRouter API error (#{response.code}): #{error_message}")
        
        Common::Result.failure(
          Common::Errors::AIProviderError.new(
            provider: 'openrouter',
            message: "API error (#{response.code}): #{error_message}",
            details: {
              status_code: response.code,
              response_body: response.body
            }
          )
        )
      end

      def estimate_tokens(text)
        # Rough estimation: 1 token â‰ˆ 4 characters
        (text.length / 4.0).ceil
      end

      def validate_configuration
        unless @config[:api_key].present?
          raise "OpenRouter API key not configured. Please set OPENROUTER_API_KEY environment variable."
        end
      end
    end
  end
end